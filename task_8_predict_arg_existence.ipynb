{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "predict whether a sentence contains claim, question, experience, experience based on claims"
      ],
      "metadata": {
        "id": "xm2ToePDwfWF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5qGiZegweKd",
        "outputId": "99fced0f-2f55-4a53-db93-c3d2612abd6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-for-sequence-classification\n",
            "  Downloading bert_for_sequence_classification-0.0.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torch!=1.10,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from bert-for-sequence-classification) (1.12.1+cu113)\n",
            "Collecting transformers>=4.2.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 13.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: openpyxl>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from bert-for-sequence-classification) (3.0.10)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-sequence-classification) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from bert-for-sequence-classification) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from bert-for-sequence-classification) (1.3.5)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-sequence-classification) (6.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.9->bert-for-sequence-classification) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bert-for-sequence-classification) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bert-for-sequence-classification) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.5->bert-for-sequence-classification) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0->bert-for-sequence-classification) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0->bert-for-sequence-classification) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0->bert-for-sequence-classification) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch!=1.10,>=1.7.1->bert-for-sequence-classification) (4.1.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->bert-for-sequence-classification) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->bert-for-sequence-classification) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->bert-for-sequence-classification) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->bert-for-sequence-classification) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->bert-for-sequence-classification) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->bert-for-sequence-classification) (4.64.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.2.0->bert-for-sequence-classification) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.2.0->bert-for-sequence-classification) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.2.0->bert-for-sequence-classification) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.2.0->bert-for-sequence-classification) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.2.0->bert-for-sequence-classification) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.2.0->bert-for-sequence-classification) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, bert-for-sequence-classification\n",
            "Successfully installed bert-for-sequence-classification-0.0.4 huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "! pip install bert-for-sequence-classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import json\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "from bert_clf import BertCLF, train_evaluate, predict_metrics, prepare_data_notebook, prepare_dataset\n",
        "from bert_clf.utils import set_global_seed"
      ],
      "metadata": {
        "id": "baxJzMTIwen8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/UP_w22/PM/task 8/data/st1_data_train_sent.csv')\n",
        "# str1_sample_train_sent.csv\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "LWsUXXWowiZl",
        "outputId": "9eaef2a6-4679-47cb-c525-53e3214793a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0 post_id subreddit_id  \\\n",
              "0                0  s1jpia     t5_2s23e   \n",
              "1                1  s1jpia     t5_2s23e   \n",
              "2                2  s1jpia     t5_2s23e   \n",
              "3                3  s1jpia     t5_2s23e   \n",
              "4                4  s1jpia     t5_2s23e   \n",
              "...            ...     ...          ...   \n",
              "127354      127354  ri83g1     t5_2r876   \n",
              "127355      127355  ri83g1     t5_2r876   \n",
              "127356      127356  ri83g1     t5_2r876   \n",
              "127357      127357  ri83g1     t5_2r876   \n",
              "127358      127358  ri83g1     t5_2r876   \n",
              "\n",
              "                                                 Sentence     Label  \\\n",
              "0       De-Nial\\nI wrote this a few years ago and just...      none   \n",
              "1       I thought I'd share...\\n\\n&#x200B;\\n\\nWhen I w...      none   \n",
              "2       Like the opening line of the old Frank Sanatra...      none   \n",
              "3       I was getting ready to graduate high school, I...      none   \n",
              "4       Growing up I was an only child with older pare...      none   \n",
              "...                                                   ...       ...   \n",
              "127354  Just how effective these drugs are and how do ...  question   \n",
              "127355                                  Edit2: Thank you.      none   \n",
              "127356  It is wonderful that there are drugs that can ...      none   \n",
              "127357                I am glad that Trikafta exists NOW.      none   \n",
              "127358  At least most people with CF don't have to hop...      none   \n",
              "\n",
              "                                               Components  \n",
              "0                                                      {}  \n",
              "1                                                      {}  \n",
              "2                                                      {}  \n",
              "3                                                      {}  \n",
              "4                                                      {}  \n",
              "...                                                   ...  \n",
              "127354  Just how effective these drugs are and how do ...  \n",
              "127355                                                 {}  \n",
              "127356                                                 {}  \n",
              "127357                                                 {}  \n",
              "127358                                                 {}  \n",
              "\n",
              "[127359 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6b176c0-04dc-4c0e-8950-5c37a86aa5d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>post_id</th>\n",
              "      <th>subreddit_id</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "      <th>Components</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>De-Nial\\nI wrote this a few years ago and just...</td>\n",
              "      <td>none</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>I thought I'd share...\\n\\n&amp;#x200B;\\n\\nWhen I w...</td>\n",
              "      <td>none</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>Like the opening line of the old Frank Sanatra...</td>\n",
              "      <td>none</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>I was getting ready to graduate high school, I...</td>\n",
              "      <td>none</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>Growing up I was an only child with older pare...</td>\n",
              "      <td>none</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127354</th>\n",
              "      <td>127354</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>Just how effective these drugs are and how do ...</td>\n",
              "      <td>question</td>\n",
              "      <td>Just how effective these drugs are and how do ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127355</th>\n",
              "      <td>127355</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>Edit2: Thank you.</td>\n",
              "      <td>none</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127356</th>\n",
              "      <td>127356</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>It is wonderful that there are drugs that can ...</td>\n",
              "      <td>none</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127357</th>\n",
              "      <td>127357</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>I am glad that Trikafta exists NOW.</td>\n",
              "      <td>none</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127358</th>\n",
              "      <td>127358</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>At least most people with CF don't have to hop...</td>\n",
              "      <td>none</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127359 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6b176c0-04dc-4c0e-8950-5c37a86aa5d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e6b176c0-04dc-4c0e-8950-5c37a86aa5d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e6b176c0-04dc-4c0e-8950-5c37a86aa5d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['Label'] = df['Label'].str.replace('claim_per_exp','Arg')\n",
        "df['Label'] = df['Label'].str.replace('claim','Arg')\n",
        "df['Label'] = df['Label'].str.replace('per_exp','Arg')\n",
        "df['Label'] = df['Label'].str.replace('question','Arg')\n",
        "df['Label'] = df['Label'].str.replace('none','O')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "13V9kdf1xEWS",
        "outputId": "7fd8696e-e812-475a-df58-75976b34d6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0 post_id subreddit_id  \\\n",
              "0                0  s1jpia     t5_2s23e   \n",
              "1                1  s1jpia     t5_2s23e   \n",
              "2                2  s1jpia     t5_2s23e   \n",
              "3                3  s1jpia     t5_2s23e   \n",
              "4                4  s1jpia     t5_2s23e   \n",
              "...            ...     ...          ...   \n",
              "127354      127354  ri83g1     t5_2r876   \n",
              "127355      127355  ri83g1     t5_2r876   \n",
              "127356      127356  ri83g1     t5_2r876   \n",
              "127357      127357  ri83g1     t5_2r876   \n",
              "127358      127358  ri83g1     t5_2r876   \n",
              "\n",
              "                                                 Sentence Label  \\\n",
              "0       De-Nial\\nI wrote this a few years ago and just...     O   \n",
              "1       I thought I'd share...\\n\\n&#x200B;\\n\\nWhen I w...     O   \n",
              "2       Like the opening line of the old Frank Sanatra...     O   \n",
              "3       I was getting ready to graduate high school, I...     O   \n",
              "4       Growing up I was an only child with older pare...     O   \n",
              "...                                                   ...   ...   \n",
              "127354  Just how effective these drugs are and how do ...   Arg   \n",
              "127355                                  Edit2: Thank you.     O   \n",
              "127356  It is wonderful that there are drugs that can ...     O   \n",
              "127357                I am glad that Trikafta exists NOW.     O   \n",
              "127358  At least most people with CF don't have to hop...     O   \n",
              "\n",
              "                                               Components  \n",
              "0                                                      {}  \n",
              "1                                                      {}  \n",
              "2                                                      {}  \n",
              "3                                                      {}  \n",
              "4                                                      {}  \n",
              "...                                                   ...  \n",
              "127354  Just how effective these drugs are and how do ...  \n",
              "127355                                                 {}  \n",
              "127356                                                 {}  \n",
              "127357                                                 {}  \n",
              "127358                                                 {}  \n",
              "\n",
              "[127359 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74ba56db-b61c-481b-94ad-ffdf4cc75d37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>post_id</th>\n",
              "      <th>subreddit_id</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "      <th>Components</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>De-Nial\\nI wrote this a few years ago and just...</td>\n",
              "      <td>O</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>I thought I'd share...\\n\\n&amp;#x200B;\\n\\nWhen I w...</td>\n",
              "      <td>O</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>Like the opening line of the old Frank Sanatra...</td>\n",
              "      <td>O</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>I was getting ready to graduate high school, I...</td>\n",
              "      <td>O</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>s1jpia</td>\n",
              "      <td>t5_2s23e</td>\n",
              "      <td>Growing up I was an only child with older pare...</td>\n",
              "      <td>O</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127354</th>\n",
              "      <td>127354</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>Just how effective these drugs are and how do ...</td>\n",
              "      <td>Arg</td>\n",
              "      <td>Just how effective these drugs are and how do ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127355</th>\n",
              "      <td>127355</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>Edit2: Thank you.</td>\n",
              "      <td>O</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127356</th>\n",
              "      <td>127356</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>It is wonderful that there are drugs that can ...</td>\n",
              "      <td>O</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127357</th>\n",
              "      <td>127357</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>I am glad that Trikafta exists NOW.</td>\n",
              "      <td>O</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127358</th>\n",
              "      <td>127358</td>\n",
              "      <td>ri83g1</td>\n",
              "      <td>t5_2r876</td>\n",
              "      <td>At least most people with CF don't have to hop...</td>\n",
              "      <td>O</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127359 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74ba56db-b61c-481b-94ad-ffdf4cc75d37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74ba56db-b61c-481b-94ad-ffdf4cc75d37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74ba56db-b61c-481b-94ad-ffdf4cc75d37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.DataFrame(columns=['Sentence', 'Label'])\n",
        "df_test = pd.DataFrame(columns=['Sentence', 'Label'])\n",
        "df_value = pd.DataFrame(columns=['Sentence', 'Label'])\n",
        "split = int(127359*0.8)\n",
        "for index, row in df.iterrows():\n",
        "  if index < split:\n",
        "    df_train = df_train.append({'Sentence': row['Sentence'], 'Label': row[\"Label\"]},ignore_index=True)\n",
        "\n",
        "  if index >= split:\n",
        "    df_test = df_test.append({'Sentence': row['Sentence'], 'Label': row[\"Label\"]},ignore_index=True)\n",
        "\n",
        "  df_value = df_value.append({'Sentence': row['Sentence'], 'Label': row[\"Label\"]},ignore_index=True)\n",
        "\n",
        "\n",
        "print(df_train.shape, df_value.shape, df_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7hUnRId0esr",
        "outputId": "f656cc6a-ad17-4a7c-f9fc-72c41502b153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101887, 2) (127359, 2) (25472, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rny24pN01-Sz",
        "outputId": "eed8b99a-82d1-4cb6-bc27-c286fc367efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101887"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['Label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRfyChJp2FOm",
        "outputId": "524b996e-52ad-40d2-a919-894401f297ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O      21594\n",
              "Arg     3878\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['Label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCZPHQZJ2ZFN",
        "outputId": "7a93f73d-6cd9-4f9a-fe03-42490d36dcc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O      86087\n",
              "Arg    15800\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_value['Label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZw7_ibs2hT-",
        "outputId": "e51a334d-fe7a-4b01-cabd-29b483be1b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O      107681\n",
              "Arg     19678\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Language Model"
      ],
      "metadata": {
        "id": "dOvqYAKD2nDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    transformer_model = dict(\n",
        "        model = \"chkla/roberta-argument\",\n",
        "        path_to_state_dict = False,\n",
        "        device = 'cuda',\n",
        "        dropout = 0.2,\n",
        "        learning_rate = 2e-5,\n",
        "        batch_size = 16,\n",
        "        shuffle = True,\n",
        "        maxlen = 128,\n",
        "    ),\n",
        "    data = dict(\n",
        "        train_data_path = df_train,\n",
        "        test_data_path = df_value,\n",
        "        text_column = \"Sentence\",\n",
        "        target_column = \"Label\",\n",
        "        random_state = 52,\n",
        "        test_size = 0.3,\n",
        "        stratify=True\n",
        "    ),\n",
        "    training = dict (\n",
        "    save_state_dict = False, # if False the model will be saved using torch.save()\n",
        "        # and should be loaded like this: model = torch.load()\n",
        "        # you will have to install the library to do so\n",
        "    early_stopping = True,\n",
        "    delta = 0.001,\n",
        "    patience = 7,\n",
        "    num_epochs = 2,\n",
        "    average_f1 = 'macro',\n",
        "    other_metrics = ['micro', 'weighted'],\n",
        "    output_dir = \"../results/\",\n",
        "    class_weight = True\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "gRi1AxYI2i2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_global_seed(seed=config['data']['random_state'])\n",
        "os.makedirs(config['training']['output_dir'], exist_ok=True)"
      ],
      "metadata": {
        "id": "lDKnfWsq2p-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(config['transformer_model']['device'])\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path=config['transformer_model'][\"model\"]\n",
        "    )\n",
        "model_bert = AutoModel.from_pretrained(\n",
        "    pretrained_model_name_or_path=config['transformer_model'][\"model\"]\n",
        ").to(device)\n",
        "\n",
        "#for param in model_bert.parameters():\n",
        "    #param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sMYg9_O23-7",
        "outputId": "b18ec3a2-1dc0-4e51-a0ba-29fe88ac7e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at chkla/roberta-argument were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at chkla/roberta-argument and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label, train_texts, valid_texts, train_targets, valid_targets = prepare_data_notebook(\n",
        "    config=config, train_df = df_train, test_df = df_value\n",
        ")"
      ],
      "metadata": {
        "id": "WiDiJ9G729jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srqq2nif3AJi",
        "outputId": "576a4d97-aed1-43db-aa30-cedaca5a843e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O', 1: 'Arg'}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertCLF(\n",
        "    pretrained_model=model_bert,\n",
        "    tokenizer=tokenizer,\n",
        "    id2label=id2label,\n",
        "    dropout=config['transformer_model']['dropout'],\n",
        "    device=device     \n",
        "    )"
      ],
      "metadata": {
        "id": "IqY9LR1H3BQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "oId6Zxts3Ctc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=float(config['transformer_model']['learning_rate']))\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "training_generator, valid_generator = prepare_dataset(\n",
        "    tokenizer=tokenizer,\n",
        "    train_texts=train_texts,\n",
        "    train_targets=train_targets,\n",
        "    valid_texts=valid_texts,\n",
        "    valid_targets=valid_targets,\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "id": "gQxNh2VZ3kpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_evaluate(\n",
        "    model=model,\n",
        "    training_generator=training_generator,\n",
        "    valid_generator=valid_generator,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=config['training']['num_epochs'],\n",
        "    average=config['training']['average_f1'],\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYHXbgRB3lpq",
        "outputId": "07970814-d1d9-4b54-bf7e-941bde047557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Epoch 1 out of 2 ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loop: 100%|██████████| 6368/6368 [35:57<00:00,  2.95it/s]\n",
            "Evaluating loop: 100%|██████████| 7960/7960 [13:46<00:00,  9.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train F1: 0.48963719872176925\n",
            "Eval F1: 0.49204893958682233\n",
            "\n",
            "Train F1 micro: 0.8447890494137353\n",
            "Eval F1 micro: 0.8458212939698493\n",
            "\n",
            "Train F1 weighted: 0.7769388191822826\n",
            "Eval F1 weighted: 0.7783980411929774\n",
            "\n",
            "==== Epoch 2 out of 2 ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loop: 100%|██████████| 6368/6368 [36:03<00:00,  2.94it/s]\n",
            "Evaluating loop: 100%|██████████| 7960/7960 [13:42<00:00,  9.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train F1: 0.4914240807642932\n",
            "Eval F1: 0.4905305359408732\n",
            "\n",
            "Train F1 micro: 0.8447491363065327\n",
            "Eval F1 micro: 0.8454920435510888\n",
            "\n",
            "Train F1 weighted: 0.777291114547064\n",
            "Eval F1 weighted: 0.7773853267782201\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing final metrics...: 100%|██████████| 7960/7960 [13:31<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Arg       0.00      0.00      0.00     19678\n",
            "           O       0.85      1.00      0.92    107681\n",
            "\n",
            "    accuracy                           0.85    127359\n",
            "   macro avg       0.42      0.50      0.46    127359\n",
            "weighted avg       0.71      0.85      0.77    127359\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8vsLta73m01",
        "outputId": "74243d20-f34e-4d91-dbf9-9d4e75e16e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertCLF(\n",
              "  (pretrained_model): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (drop): Dropout(p=0.2, inplace=False)\n",
              "  (act): LogSoftmax(dim=1)\n",
              "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "for i,j in zip(df_test['Sentence'], df_test['Label']):\n",
        "    preds.append([model.predict(i), j, i])"
      ],
      "metadata": {
        "id": "USxUasW24ISN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "for i in preds:\n",
        "    pred.append(i[0])\n",
        "\n",
        "true = []\n",
        "for m in preds:\n",
        "    true.append(m[1])"
      ],
      "metadata": {
        "id": "Ii4v6ikC4MCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# roberta- arg on sentence level, USElecDeb corpus, task 1\n",
        "\n",
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(true, pred, target_names=target_names, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH1j87Jn4Pla",
        "outputId": "25e39b94-679f-4c6e-8571-d3e89ed69f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0      0.000     0.000     0.000      3878\n",
            "     class 1      0.848     1.000     0.918     21594\n",
            "\n",
            "    accuracy                          0.848     25472\n",
            "   macro avg      0.424     0.500     0.459     25472\n",
            "weighted avg      0.719     0.848     0.778     25472\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# O      107681\n",
        "# Arg     19678"
      ],
      "metadata": {
        "id": "fiBgn5y74Q0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}